{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Exercise 4 - XGBoost\n\n## Group *ID* : $2207$\n\n### Members\n\n- Pietro Cappelli\n    - e-mail: pietro.cappelli@studenti.unipd.it\n    - ID: 2058332\n- Alberto Coppi\n    - e-mail: alberto.coppi@studenti.unipd.it\n    - ID: 2053063\n- Giacomo Franceschetto\n    - e-mail: giacomo.franceschetto@studenti.unipd.it\n    - ID: 2053348\n- Nicolò Lai\n    - e-mail: nicolo.lai@studenti.unipd.it\n    - ID: 2064377 \n### Author contribution statement\n\nAlbicoppi did the entire exercise on his own. ",
   "metadata": {
    "cell_id": "d9996f3d1f894937b37c0fe967f782f1",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 630.75
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Import libraries and custom modules",
   "metadata": {
    "cell_id": "0a35e4b374874d4a8f8d9a20f60610c8",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "221e7e1ca1bd427681e147064b5333de",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8b3aba13",
    "execution_start": 1648569588057,
    "execution_millis": 6620,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 477
   },
   "source": "import numpy             as np\nimport matplotlib.pyplot as plt\nimport seaborn           as sns\nimport pandas            as pd\n\nimport keras\nimport tensorflow as tf\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten #, Reshape\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, GlobalMaxPooling1D\nfrom keras        import regularizers, initializers\nfrom scipy.signal import detrend\nfrom sklearn.metrics import accuracy_score\n\nfrom tsfresh import extract_features\nfrom xgboost import XGBClassifier, plot_tree\n\n%load_ext autoreload\n%autoreload 1\n%aimport generate_data \nfrom split import splitting\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "f87355e4f8314eda80c9c2f54f427f86",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5255e2a9",
    "execution_start": 1648569594687,
    "execution_millis": 9,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 567
   },
   "source": "def standardize_sample(x):\n    \"\"\"rescale data sample-wise: for each sample subtract the mean and divide by std\"\"\"\n\n    xm = x.mean(axis=1)\n    for i in range(N):\n        x[i] = x[i]-xm[i]\n    vm = np.std(x, axis=1)\n    x = x/vm[:, np.newaxis]\n    \n    return x\n\ndef standardize_all(x):\n    \"\"\"rescale data semi-sample-wise: subtract the mean sample-wise then divide by the total std\"\"\"\n\n    xm = x.mean(axis=1)\n    for i in range(N):\n        x[i] = x[i]-xm[i]\n    vm = np.std(x)\n    x = x/vm\n\n    return x\n\n\ndef keras_reshape(x):\n    \"\"\"reshape data to fit keras shape requirements\"\"\"\n    \n    return x.reshape(x.shape[0], x.shape[1], 1)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Data parameters",
   "metadata": {
    "cell_id": "a2df9d992c8a488ba7c95b309b96789f",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a0ad4cec14d8444bbf9a5815845b7ca6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f132adb4",
    "execution_start": 1648569594697,
    "execution_millis": 12,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 315
   },
   "source": "N_list = [20, 50, 100, 150, 200, 250, 300, 400, 500]\nN_stat = 10\n\nseed_list = [12345 + i for i in range(0, N_stat)]\n\n# time series data parameters\nL       = 60\nZ       = 12\nA       = 500\nDX      = 50\nbias    = 5\nn_class = 3  \n\ninput_shape = (L,1)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Best performing CNN ",
   "metadata": {
    "cell_id": "432ea297d6334b38a513c35110eddddc",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d2a62bacbb524d689fe70a7952b9e40d",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "51e4bf59",
    "execution_start": 1648569594710,
    "execution_millis": 419,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1128
   },
   "source": "reg = regularizers.l2(0.02)\nini = keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n\nNF    = 5 # number of filters\n\nmodel_cnn = Sequential(name=\"giampaolo\")\n\n\nmodel_cnn.add(\n    Conv1D(\n        filters            = 6, \n        kernel_size        = 3, \n        kernel_initializer = ini,\n        kernel_regularizer = reg,\n        activation         = \"relu\",\n        input_shape        = input_shape\n    )\n)\nmodel_cnn.add(Conv1D(filters=8, kernel_size=9, activation=\"relu\"))\nmodel_cnn.add(Conv1D(filters=4, kernel_size=3, activation=\"relu\"))\nmodel_cnn.add(GlobalMaxPooling1D()) \nmodel_cnn.add(Flatten())\nmodel_cnn.add(Dense(3, activation=\"softmax\"))\n\nprint(model_cnn.summary())\n\nopt = tf.keras.optimizers=\"Adam\"\n\n# save initial weights\ninitial_weights = model_cnn.get_weights()\n\n# compile\nmodel_cnn.compile(loss=keras.losses.categorical_crossentropy, optimizer=opt, metrics=[\"accuracy\"])",
   "outputs": [
    {
     "name": "stdout",
     "text": "Model: \"giampaolo\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv1d (Conv1D)             (None, 58, 6)             24        \n                                                                 \n conv1d_1 (Conv1D)           (None, 50, 8)             440       \n                                                                 \n conv1d_2 (Conv1D)           (None, 48, 4)             100       \n                                                                 \n global_max_pooling1d (Globa  (None, 4)                0         \n lMaxPooling1D)                                                  \n                                                                 \n flatten (Flatten)           (None, 4)                 0         \n                                                                 \n dense (Dense)               (None, 3)                 15        \n                                                                 \n=================================================================\nTotal params: 579\nTrainable params: 579\nNon-trainable params: 0\n_________________________________________________________________\nNone\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## CNN performance",
   "metadata": {
    "cell_id": "c1fb4ccb98ba4258916ab9d746fe0ecd",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "314175163255450f807d896a5b1b341a",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1e1ca7f0",
    "execution_start": 1648569595124,
    "execution_millis": 5,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 783
   },
   "source": "class Dataset:\n\n    def __init__(self, x, y, train_test_frac, valid_train_frac):\n\n        self.x = x\n        self.y = y\n\n        N = x.shape[0]\n\n        self.N_train = int(N*train_test_frac*(1-valid_train_frac))\n        self.N_valid = int(N*train_test_frac*valid_train_frac)\n        self.N_test  = int(N*(1-train_test_frac))\n\n        self.train_idx = self.N_train\n        self.valid_idx = self.N_train+self.N_valid\n        self.test_idx  = self.N_train+self.N_valid+self.N_test\n\n    @property\n    def xtrain(self):\n        return self.x[0:self.train_idx]\n\n    @property\n    def ytrain(self):\n        return self.y[0:self.train_idx]\n\n    @property\n    def xvalid(self):\n        return self.x[self.train_idx:self.valid_idx]\n\n    @property\n    def yvalid(self):\n        return self.y[self.train_idx:self.valid_idx]\n\n    @property\n    def xtest(self):\n        return self.x[self.valid_idx:self.test_idx]\n\n    @property\n    def ytest(self):\n        return self.y[self.valid_idx:self.test_idx]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0bc93fa81c3c43e9b13556ff138f1cb1",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "18b73f38",
    "execution_start": 1648569595125,
    "execution_millis": 5,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 459
   },
   "source": "def get_df(x):\n    '''Build input dataframe for given data series\n    Input:\n    var = array of time series, (#samples,time,1)\n    Return:\n    df = dataframe ready for features extraction\n    '''\n    \n    #N = #samples, t = timesteps\n    N, t = x.shape[0], x.shape[1]\n    #build id columns\n    id_col = np.repeat(np.arange(N),t)\n    #build time columns\n    time_col = np.tile(np.arange(t),N)\n    #build var columns\n    x_col = x.flatten()\n      \n    #build dict for df\n    x_dict = {'id':id_col,'time':time_col,'value':x_col}\n        \n    #return dataframe\n    return pd.DataFrame(x_dict)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "cd8b84ac7e44415d9bad717526b7bd05",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "43f9184e",
    "execution_start": 1648569595189,
    "execution_millis": 78581,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 3275
   },
   "source": "BATCH_SIZE = 250\nEPOCHS     = 100\n\n# list for different N\n# for each N create a dictionary\n# dictionary contains:\n#  - cnn mean accuracy\n#  - cnn std\n#  - xgboost mean accuracy\n#  - xgboost std\n\nperformance_list = []\n\nfor N in N_list:\n    cnn_performance     = []\n    xgb_performance     = []\n    for i, seed in zip(range(N_stat), seed_list):\n    \n        fname = generate_data.generate_data(N=N, seed=seed)\n\n        # load data\n        x = np.loadtxt(\"DATA/x_\"+fname, delimiter=\" \",dtype=float)\n        # load categories\n        c = np.loadtxt(\"DATA/y_\"+fname, dtype=int)\n        # build time grid\n        t_grid = np.arange(0, L*N).reshape(N, L)\n        # format labels from categories\n        y = np.zeros((N, n_class))\n        for i in range(N):\n            y[i][c[i]] = 1\n\n        # data rescaling\n        x_detrend = detrend(x)\n        x = standardize_all(x_detrend)\n\n        ##### ---- CNN\n        dataset = Dataset(x, y, train_test_frac=0.7 ,valid_train_frac=0.3)\n\n        x_train = dataset.xtrain\n        y_train = dataset.ytrain\n        x_valid = dataset.xvalid\n        y_valid = dataset.yvalid\n        x_test  = dataset.xtest\n        y_test  = dataset.ytest\n\n        # reshape for keras\n        x_train = keras_reshape(x_train)\n        x_valid   = keras_reshape(x_valid)\n\n        # reset model\n        model_cnn.set_weights(initial_weights)\n\n        # compile\n        model_cnn.compile(loss=keras.losses.categorical_crossentropy, optimizer=opt, metrics=['accuracy'])\n\n        # fit CNN model\n        fit = model_cnn.fit(\n            x_train, \n            y_train, \n            batch_size      = BATCH_SIZE,\n            epochs          = EPOCHS, \n            validation_data = (x_valid, y_valid),\n            verbose         = 0, \n            shuffle         = True\n        )\n\n        # evaluate model on test dataset\n        test = model_cnn.evaluate(\n            x_test, y_test,\n            batch_size      = BATCH_SIZE,\n            verbose         = 0,\n        )\n\n        test_accuracy = test[1]\n\n        cnn_performance.append(test_accuracy)\n        ##### ----\n\n\n        ##### ---- XGB boost\n        df = get_df(x)\n\n        #extract features\n        x_features = extract_features(\n            df, #our dataframe\n            column_id='id', #sample id, from 0 to N\n            column_sort='time', #timestep, from 0 to t\n            column_kind=None, #we have only one feature\n            column_value='value', #value of input \n            n_jobs=4 #number of cores\n        ) \n\n        #remove columns with NaN or inf\n        #x_features.replace([np.inf, -np.inf], np.nan)\n        #x_features = x_features.dropna(axis='columns') \n\n        #split data into training and validation\n        dataset = Dataset(x_features, y, train_test_frac=0.7 ,valid_train_frac=0.3)\n\n        x_train = dataset.xtrain\n        y_train = dataset.ytrain\n        x_valid = dataset.xvalid\n        y_valid = dataset.yvalid\n        x_test  = dataset.xtest\n        y_test  = dataset.ytest\n\n        #rescale: in each feature, remove average and divide by std\n        if True: \n            train_avg = np.mean(x_train,axis=0)\n            test_avg  = np.mean(x_test,axis=0)\n            x_train -= train_avg\n            x_test  -= test_avg\n\n            train_std = np.std(x_train,axis=0)\n            test_std  = np.std(x_test,axis=0)\n            x_train  /= train_std\n            x_test  /= test_std \n\n        #define parameters for xgboost\n        params = {'max_depth':6,'min_child_weight':1,\\\n                'learning_rate':0.3,'use_label_encoder':False}\n\n        #build model with given params\n        model_xgb = XGBClassifier(**params)\n\n        #fit\n        model_xgb.fit(x_train.values, np.argmax(y_train, axis=1))  \n\n        \n        #predict labels on test set\n        y_pred_test = model_xgb.predict(x_test)\n\n        #compute accuracies # ci va l'argmax anche qui?\n        xgb_performance.append(accuracy_score(np.argmax(y_test, axis=1),y_pred_test))\n\n    performance_list.append(\n        {\n            \"cnn_mean_accuracy\" : np.mean(cnn_performance),\n            \"cnn_std_accuracy\"  : np.std(cnn_performance),\n            \"xgb_mean_accuracy\": np.mean(xgb_performance),\n            \"xgb_std_accuracy\"  : np.std(xgb_performance),\n        }\n    )\n ",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.96it/s]\n[16:00:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.64it/s]\n[16:00:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.92it/s]\n[16:00:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:01<00:00, 10.67it/s]\n[16:00:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.65it/s]\n[16:00:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.29it/s]\n[16:00:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:01<00:00, 10.56it/s]\n[16:00:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.36it/s]\n[16:00:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.77it/s]\n[16:01:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:01<00:00, 11.58it/s]\n[16:01:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 17/17 [00:04<00:00,  4.06it/s]\n[16:01:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 17/17 [00:04<00:00,  3.97it/s]\n[16:01:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 17/17 [00:04<00:00,  4.05it/s]\n[16:02:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 17/17 [00:04<00:00,  3.94it/s]\n[16:02:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 17/17 [00:04<00:00,  4.05it/s]\n[16:03:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 17/17 [00:05<00:00,  2.92it/s]\n[16:03:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 17/17 [00:04<00:00,  3.96it/s]\n[16:04:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 17/17 [00:04<00:00,  4.00it/s]\n[16:05:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 17/17 [00:04<00:00,  4.03it/s]\n[16:05:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 17/17 [00:04<00:00,  4.01it/s]\n[16:06:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s]\n[16:06:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:08<00:00,  2.37it/s]\n[16:07:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:08<00:00,  2.39it/s]\n[16:08:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s]\n[16:08:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:08<00:00,  2.42it/s]\n[16:09:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:08<00:00,  2.37it/s]\n[16:10:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:08<00:00,  2.39it/s]\n[16:10:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:08<00:00,  2.42it/s]\n[16:11:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:08<00:00,  2.42it/s]\n[16:12:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:08<00:00,  2.39it/s]\n[16:12:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 19/19 [00:12<00:00,  1.55it/s]\n[16:13:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 19/19 [00:12<00:00,  1.57it/s]\n[16:14:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 19/19 [00:12<00:00,  1.55it/s]\n[16:15:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 19/19 [00:12<00:00,  1.57it/s]\n[16:15:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 19/19 [00:12<00:00,  1.54it/s]\n[16:16:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 19/19 [00:12<00:00,  1.53it/s]\n[16:17:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 19/19 [00:12<00:00,  1.54it/s]\n[16:18:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 19/19 [00:12<00:00,  1.51it/s]\n[16:18:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 19/19 [00:12<00:00,  1.52it/s]\n[16:19:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 19/19 [00:12<00:00,  1.53it/s]\n[16:20:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\n[16:21:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n[16:22:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n[16:23:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n[16:24:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n[16:24:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n[16:25:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\n[16:26:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\n[16:27:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n[16:28:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:18<00:00,  1.05it/s]\n[16:29:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:20<00:00,  1.03s/it]\n[16:30:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:20<00:00,  1.02s/it]\n[16:31:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:20<00:00,  1.03s/it]\n[16:32:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:20<00:00,  1.02s/it]\n[16:33:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:20<00:00,  1.02s/it]\n[16:34:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:20<00:00,  1.02s/it]\n[16:35:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:20<00:00,  1.03s/it]\n[16:36:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:23<00:00,  1.18s/it]\n[16:37:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:20<00:00,  1.03s/it]\n[16:38:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:20<00:00,  1.03s/it]\n[16:39:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:24<00:00,  1.24s/it]\n[16:40:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFeature Extraction: 100%|██████████| 20/20 [00:24<00:00,  1.23s/it]\n[16:41:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "KernelInterrupted",
     "evalue": "Execution interrupted by the Jupyter kernel.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# non ho voglia di aspettare che finisca, è infinito cazzo\nimport pickle\nwith open(\"perfromance_list\", ‘wb’) as f:\n    pickle.dump(perfromance_list, f)",
   "metadata": {
    "cell_id": "7d135ab028fa489b87d01d20931677b7",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b663dc36",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 135
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "bc57f8f441b244ecbf81bb5c837ceccd",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "efaae623",
    "execution_start": 1648567029237,
    "execution_millis": 2,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 176,
    "deepnote_output_heights": [
     78.796875
    ]
   },
   "source": "performance_list",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 33,
     "data": {
      "text/plain": "[{'cnn_mean_accuracy': 0.75,\n  'cnn_std_accuracy': 0.08333331346511841,\n  'xgb_mean_accuracy': 0.75,\n  'xgb_std_accuracy': 0.08333333333333337}]"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "78bf1ee6d7b149af986163336d84daba",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "61a654a8",
    "execution_start": 1648566906397,
    "execution_millis": 2,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 118.1875,
    "deepnote_output_heights": [
     21.1875
    ]
   },
   "source": "xgb_performance",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 31,
     "data": {
      "text/plain": "[0.8333333333333334, 0.6666666666666666]"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "91bb7ba10bd24799b4627adcd2dbb93b",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 66
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=452e6836-b7b8-455e-80d9-d4109b6baddf' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "712e75d3-c827-41fd-8bcd-a56f35ad213c",
  "deepnote_execution_queue": []
 }
}